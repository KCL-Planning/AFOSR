====================================================================
====================================================================
====================================================================
ABSTRACT AS SENT TO RW:

Explainable AI Planning (XAIP): Overview and the Case of
Contrastive Explanation

Model-based approaches to AI are well suited to explainability in
principle, given the explicit nature of their world knowledge and
of the reasoning performed to take decisions. AI Planning in
particular is relevant in this context as a generic approach to
action-decision problems. Indeed, explainable AI Planning (XAIP)
has received interest since more than a decade, and has been taking
up speed recently along with the general trend to explainable AI.

The lecture offers an introduction to the nascent XAIP area. The
first half provides an overview, categorizing and illustrating the
different kinds of explanation relevant in AI Planning, and placing
previous work in this context. The second half of the lecture goes
more deeply into one particular kind of XAIP, contrastive
explanation, aimed at answering user questions of the kind "Why do
you suggest to do A here, rather than B (which seems more
appropriate to me)?". Answers to such questions take the form of
reasons why A is preferrabe over B. Covering recent work by the
lecturers towards this end, we set up a formal framework allowing
to provide such answers in a systematic way; we instantiate that
framework with the special case of questions about goal-conjunction
achievability in oversubscription planning (where not all goals can
be achieved and thus a trade-off needs to be found); and we discuss
the compilation of more powerful question languages into that
special case. Linking to the state of the art in research on
effective planning methods, we briefly cover recent techniques for
nogood learning in state space search, as a key enabler to
efficiency in the suggested analyses.



====================================================================
====================================================================
====================================================================
OUTLINE PART 1

=== Intro



====================================================================
====================================================================
====================================================================
OUTLINE PART 2

Follow AAAI submission. In framework section, point to the more
general definitions where relevant. Add some material on nogood
learning if time. Hence:

=== Introduction (AAAI sec 1: motivate approach)

=== OSP (definition, example)

=== Explanation Framework (AAAI sec 3)

=== Computing Explanations (AAAI sec 4, keep brief, explain as finding
borderline in lattice of goal formulas; briefly give empirical results)

=== Compilations (AAAI sec 5, start with LTL and known compilations,
mention prelim results, then proceed to special case action-set
properties with very effective compilation; briefly give empirical
results)

=== NoGood Learning in State Space (use previous slides and Marcel's
material, scale level of detail depending on expected timing)
