
% pgf settings: shrink the tick labels a bit
\pgfplotsset{every tick label/.append style={font=\scriptsize}}

\newcommand{\scatterplotsize}{8cm}
\newcommand{\scatterplotxlabelshift}{1.5ex}
\newcommand{\scatterplotylabelshift}{-3ex}




\section{Experiments}
\label{experiments}

%% \joerg{1.5--2 page: similar to ijcai version; expliucotly distinguish 
%% global vs local, add results for local in both ipc and action-set prop
%% experiments}

We implemented our approach in Fast Downward
(FD) \cite{helmert:jair-06}. We evaluate it, in turn, on IPC
benchmarks modified for OSP planning, and on a selection of IPC
benchmarks extended with action-set properties.

The base planner called by our SysS and SysW algorithms on each search
node runs forward search using
\hff\ \cite{hoffmann:nebel:jair-01}, optionally 
with nogood learning.
%
%% The base planner configurations, used to solve/prove unsolvability
%% of a meta search node, are greedy best first search with $\hff$ and
%% preferred operators ($hff$) and conjunction learning $\hc$ with
%% $\hff$ as its base heuristic. \rebecca{ask Marcel how it is
%% called} \rebecca{Modification of hC to find deadends with an cost
%% bound}
%
The experiments were run on a cluster of Intel E5-2660 machines
running at 2.20 GHz, with time (memory) cut-offs of 30 minutes (4
GB).



 
\subsection{IPC-Based OSP Benchmarks}

% The net-benefit benchmarks don't give us anything new (the ones we
% could use are adopted from IPC ben chmarks anyhow).
%
%% \joerg{Rebecca/Michael: check out the IPC net-benefit benchmarks. Reviewers may naturally expect us to experiment with those, given our strong focus on oversubscription planning (actually this question came up in the discussion with the NASA guys yesterday). In the net-bnefit benchmarks, goal facts have rewards which we don't need. The question is whether, stripping away these rewards and imposing a plan-cost bound, we would get benchmarks not already covered by bour IPC experiments anyway. If the answer is "no", we can just say so in the paper. If the answer is "yes", it would be good (though probably not absolitely necessary) to experiment with these domains as well. In any case, we should know what the answer is.}

We modified all optimal-planning STRIPS IPC domains up to
IPC'18. Following Domshlak and
Mirkis \shortcite{domshlak:mirkis:jair-15}, for each benchmark task
$(\vars,\acts,\cost,\init,\goal)$ with optimal plan cost $C$ we
obtained three OSP tasks by setting the cost bound to $b = x * C$
where $x \in \{0.25, 0.5, 0.75\}$, with soft goals only \ie\
$\goalsoft = \goal$ and $\goalhard=\emptyset$. Our benchmark set
consists of 46 domains, and contains those tasks whose optimal plan
cost is known \joerg{add cite/reference}, and where the number of goal
facts is $< 32$.
%
%% ; the latter is an artifact of our current implementation that
%% could be overcome in principle, though computing all MUGS for that
%% many goals is presumably typically infeasible anyway.
%
% Joerg: said up front where it belongs
%
%% We extended conjunction learning \cite{steinmetz:hoffmann:ai-17} to
%% deal with cost bounds, thus enabling nogood learning and transfer in
%% SysS and SysW.
%
We consider conjunction learning as an algorithmic
enhancement, \joerg{update} but not trap learning as that cannot deal
with OSP cost bounds.
%
%% In what follows, we consider first global explanations, then discuss
%% how the picture changes for local explanations.


\subsubsection{IPC Global Explanations}

\setlength{\tabcolsep}{2pt}
\renewcommand{\arraystretch}{0.8}
\begin{figure*}[h!]
	\tiny
	%\centering \input{tables/coverage_IPC.tex} 
	\centering \input{tables/coverage_IPC_max_mugs.tex} 
        \vspace{-0.2cm}
	\caption{Results on IPC benchmarks modified for oversubscription planning. Reference Points: related classical planning tasks (see text). Coverage: of our MUGS algorithms SysS and SysW, with vs.\ without conjunction learning \hc. \#MUGS: average/maximum number of MUGS, indicating explanation size (see text). Search Tree Fraction: fraction of worst-case search tree explored. Best performance in each part shown in \textbf{boldface}. Cost bounds set to $x$ times optimal cost. \joerg{do our results get better with trap learning?}}
	\label{table:coverage_ipc}
        \vspace{-0.5cm}
\end{figure*}

Figure~\ref{table:coverage_ipc} shows our data for global
explanations, \ie, computing all MUGS. 
%
Note first that the figure includes \#MUGS data, corresponding to
global explanation size. As the data shows, that size is often small,
of a scale that can be feasible for human inspection.
%
% joerg: previous text, when we didnt yet have separate analysis/data
% for local.
%
%% Observe that, if the user asks a question ``Why $r$ rather than
%% $p$?'', the answer are the properties entailed by $p$, represented
%% here through the smallest conjunctions excluded by $p$. The number of
%% such conjunctions is at most the number of MUGS. So \#MUGS corresponds
%% to worst-case answer/explanation size.
%
%% (Taking the maximum rather than average per domain, the average across
%% domains is 113.4, 45.6, 24.0 for $x=0.25, 0.5, 0.75$ respectively.)
%% \rebecca{discus new mugs max columns}
%
%% The average MUGS size for a cost bound of 0.25 is small (1.32). It
%% is often the case that for these problems, you cannot reach any of
%% the goal facts. In that case, the MUGS will be the goals.

The rest of the the figure focuses on computational performance. As a
measure to compare against, we use classical planning and OSP as
reference points. The \hlmcut\ column gives coverage for \astar\
with \hlmcut\ \cite{helmert:domshlak:icaps-09} run on the original IPC
instance without a cost bound, as a comparison to solvable optimal
planning. The OSP column gives coverage for the most recent OSP
planner \cite{katz:etal:icaps-19}.
%
It is expected that our algorithms, solving a more complex problem,
will perform worse than the reference points. The question
is, \emph{how much worse?}

\joerg{update to OSP comparison}
%
As a short summary of the answer provided by
Figure~\ref{table:coverage_ipc} to that question, compared to
the \hlmcut\ reference point, taking the per-domain best of our four
algorithm configurations, for $x=0.25$ we get equal coverage in 36 of
the 46 domains, and in that sense are ``not much worse'' than optimal
planning. For larger cost bounds, the base planner's forward search
space is larger, and accordingly our analysis becomes harder. For
$x=0.5$ we get equal coverage in 23 domains, for $x=0.75$ in 13. The
comparison to the \hc\ proving-unsolvability reference points is
qualitatively similar, with equal coverage in 38, 25, and 20 domains
for $x=0.25, 0.5, 0.75$ respectively. Overall, it seems fair to say
that our analyses can be feasible in many cases, in the sense of not
being more infeasible than the most closely related classical planning
problems. 

Both SysS and SysW suffer from larger cost bounds, but that is less so
for SysW, due to its smaller search tree when solvable goal sets are
large, as shown in the rightmost part of
Figure~\ref{table:coverage_ipc}. Conjunction learning is moderately
beneficial.
%
%% Comparing SysS and SysW, with cost bound 0.25 both show better
%% coverage in 4 domains. Among those domains, in \woodworking\
%% and \openstacks\ SysS explores a much smaller fraction of the
%% meta-search tree than SysW (0.02 vs. 0.99 and 0.06 vs. 0.99). With
%% a cost bound of 0.5 SysW has better coverage in more domains (8
%% vs. 6). With cost 0.75 both show better coverage in 7
%% domains. Although SysW explores the smaller fraction of the
%% meta-search tree, SysS still demonstrates better coverage
%% overall. \rebecca{In this setting, finding a plan is easier than
%% proving unsolvability?}
%
%% The table shows that $\hc$ is useful with SysW, but for SysS only
%% with cost bound is $0.25$.



\subsubsection{IPC Local Explanations}

For local explanations -- the entailments of a soft-goal conjunction
$\bigwedge_{g \in A} g$ -- we set up an experiment where we used only
benchmark instances with $\geq 6$ goals, scaled question size $|A|$
from $1$ to $5$, and randomly selected $10$ questions of each
size. Figure~\ref{fig:ipc-local} shows the data for the most
challenging cost bound $0.75$. As expected, both computational effort
and explanation size decrease with $|A|$, \ie, with how specific the
user question is.

\begin{figure}[ht]
\small
\centering

\begin{tabular}{cc}
\begin{minipage}{0.25\textwidth}
\resizebox{!}{3.0cm}{
\input{data/IPC_cost_bound/search_time_coverage_mix_bu.tex}
}
\end{minipage} &
\begin{minipage}{0.25\textwidth}
\resizebox{!}{3.0cm}{
\input{data/IPC_cost_bound/mugs_bu_all.tex}
}
\end{minipage}
\end{tabular}
\vspace{-0.3cm}
\caption{\label{fig:ipc-local} Left: coverage (dashed, right y-axis) and
average runtime (solid, left y-axis) for SysS, as a function of
$|A|$. Right: average \#MUGS as a function of $|A|$. Averages taken
over instances where all questions were solved.}
\vspace{-0.6cm}
\end{figure}

%\input{tables/all_points.tex}
%\input{data/IPC_cost_bound/search_time_coverage_mix_td.tex}

This behaviour is very consistent across domains, and is much stronger
still in some cases. Of the 42 domains that have at least one instance
with $\geq 6$ goals, 7 domains have much stronger ($> 40\%$) increases
in coverage from $|A|=1$ to $|A|=5$,
%
%% For coverage, Depots, Gripper, NoMystery, Parking, Pathways,
%% Scanalyzer, Trucks
%
and in 24 domains average runtime decreases by at least one order of
magnitude. 
%
%% runtime: 1 order blocks, drive, floortile, elevators, grip, ged,
%% mic, log, mov, pipest, pipesnt, sat, psr, soko, stor, tetr, tpp,
%% transp, wood; 2 orders depots, nomystery, parcprinter, scan, trucks
%
The average number of MUGS for $|A|=5$ is less than $5$ in all
domains, including ones like Floortile, Gripper, Miconic, Woodworking
where global explanations can be large as shown above in
Figure~\ref{table:coverage_ipc}.













%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% SUBMISSION VERSION

\ifdefined\suppflagdefined

\else

\subsection{Action-Set Properties}

To evaluate the use of our framework with more complex plan
properties, beyond goal facts, we experimented with the compilation of
action-set properties as per Section~\ref{compilation}. We selected
four IPC domains for extension with action-set properties, namely
NoMystery, Rovers, and TPP as considered in resource-constrained
planning \cite{nakhost:etal:icaps-12}, where minimum resource
requirements are known as per available problem generators; plus the
Blocksworld as an intuitively rather differently structured domain. In
all four domains, we use discrete resource consumption encoded into
the STRIPS model, enabling the use of trap
learning \cite{steinmetz:hoffmann:ijcai-17} which turns out to be
highly beneficial here.

In Blocksworld, we include two gripper hands and the action-set
properties ask whether a given gripper is used to pick up a given
block, or to stack a given pair of blocks. In NoMystery, the
properties are as in our illustrative example. In Rovers, the
properties ask whether a given rover or camera is used for a given
observation. In TPP, they ask whether given road segments are used,
and whether given goods are bought at given markets. In all cases, we
vary the number of action-set properties between 1 and 10. We fix the
original goal facts as hard goals, and we set the available resources
to $x \in \{1.0,1.5, 2.0\}$ times the minimum needed to allow for
costlier plans satisfying some of the properties.

We created benchmark tasks with size parameters around the borderline
of feasibility. As reference points, we ran \astar\ with \hlmcut\
respectively trap learning on tasks where all (original goal facts
plus) action-set properties are hard goals.




For lack of space, we only summarize our data.\footnote{For AAAI'20
review, the complete data is available in supplementary material at
{\scriptsize \url{https://www.dropbox.com/sh/boq29booqajj7ab/AACbKpiR6jdbeEzCldrvLfk4a?dl=0}}}
%
\joerg{this summary is extremely underwhelming, focusing exclusively 
on computational aspects, and saying ``not totally infeasible''. we
need something that highlights to the reviewer that/how this new form
of analysis is exciting and useful. examples of concrete analysis
results?}
%
In Blocksworld and TPP, our techniques are moderately competitive with
the reference points. In NoMystery, they are vastly inferior. In
Rovers, they vastly surpass \hlmcut, matching the coverage of the
trap-learning reference point. Overall, it seems fair to say that our
analysis here is not exceedingly infeasible compared to related
classical planning problems.




\fi

%%%% END SUBMISSION VERSION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% PRE-FINAL AND SUPPLEMENTARY MATERIAL VERSION

\ifdefined\suppflagdefined

\subsection{Action-Set Properties}

\begin{figure*}[htb]
\centering\centering
%\input{data/action_set_properties/domain_selection.tex}
\includegraphics{data/action_set_properties/barchart/barchart.pdf}
\vspace{-0.6cm}
\caption{Coverage results on IPC benchmarks extended with action-set properties.}
\label{fig:barcharts}
\vspace{-0.2cm}
\end{figure*}

To evaluate the use of our framework with more complex plan
properties, beyond goal facts, we experimented with the compilation of
action-set properties as per Section~\ref{compilation}. We selected
four IPC domains for extension with action-set properties, namely
NoMystery, Rovers, and TPP as considered in resource-constrained
planning \cite{nakhost:etal:icaps-12}, where minimum resource
requirements are known as per available problem generators; plus the
Blocksworld as an intuitively rather differently structured domain. In
all four domains, we use discrete resource consumption encoded into
the STRIPS model, enabling the use of trap
learning \cite{steinmetz:hoffmann:ijcai-17} which turns out to be
highly beneficial here.

In Blocksworld, we include two gripper hands and the action-set
properties ask whether a given gripper is used to pick up a given
block, or to stack a given pair of blocks. In NoMystery, the
properties are as in our illustrative example
(Section~\ref{illustrative-example}). In Rovers, the properties ask
whether a given rover or camera is used for a given observation. In
TPP, they ask whether given road segments are used, and whether given
goods are bought at given markets. In all cases, we vary the number of
action-set properties between 1 and 10. We fix the original goal facts
as hard goals, and we set the available resources to $x \in \{1.0,1.5,
2.0\}$ times the minimum needed to allow for costlier plans satisfying
some of the properties.

We created benchmark tasks with size parameters around the borderline
of computational feasibility for our analyses, given our time/memory
limits. In Blocksworld, we used 5 -- 8 blocks; in NoMystery, our tasks
have 2 trucks, 6 locations, and 4 -- 7 packages; in Rovers, they have
2 rovers, 5 waypoints, and 4 -- 7 science objectives; in TPP, we use 5
markets, 1 depot, and 4 -- 7 goods. In all domains, we vary the number
of goal facts (and associated objects) between 4 and 7. We create 10
base instances for each size-parameter setting, which are then
modified for our experiments with different initial resource levels,
and action-set properties to be considered.
%
%% \begin{enumerate}
%% \item The resource constrained \textit{rovers} domain. Problems were generated with 2 rovers, 5 waypoints. Action properties are to use a specific rover for a sample or an observation, or to use a specific camera for an observation. 
%% \item The \textit{blocksworld} domain with 2 grippers, modified such that picking up or unstacking a block costs high or low energy depending upon which gripper is used. Problems were generated scaling from 3 to 10 blocks. Action properties are to use a specific gripper to pick up a specific block, or to use any gripper to stack a specific pair of blocks at any point in the plan.
%% \item The resource constrained \textit{TPP} domain. Problems were generated with 5 markets and 1 depot. Properties are to use or not use particular road segments, and preferred markets for goods.
%% \item The resource constrained \textit{nomystery} domain, described in the example. Problems were generated with 6 locations and 2 trucks.
%% \end{enumerate}

To have some comparison measure for performance, again we use
classical-planning reference points, based on \astar\ with \hlmcut,
and on search with trap learning, respectively. We now run these
reference points on tasks where all (original goal facts plus)
action-set properties are hard goals. These tasks may be solvable (in
which case \astar\ with \hlmcut\ tends to be better) or unsolvable (in
which case trap learning tends to be better). The configurations of
our own algorithm are SysS and SysW as before, now with vs.\ without
trap learning (and transfer).

Figure~\ref{fig:barcharts} shows the coverage data. To give an
overview, we show one row per domain, fixing the number of hard goals
at the feasibility borderline. Smaller numbers of goal facts tend to
be quite easy, larger ones mostly infeasible, with variance depending
on the domain and algorithm.
Appendix~\ref{data-action-set-properties} gives complete data for each
of the four domains.

In Blocksworld, the best of our techniques are moderately competitive
with the \hlmcut\ reference point (which starts to lose coverage when
one more block is added). They match the performance of the other
reference point for $x=1.0$, and surpass it for larger $x$ where trap
learning incurs a prohibitive overhead. NoMystery is the most
problematic domain in terms of performance, with all our techniques
lagging far behind the two reference points. In Rovers,
though, \astar\ with \hlmcut\ is much less effective than our
techniques, which match the full coverage of the trap-learning
reference point. TPP is similar to Blocksworld in that our techniques
are moderately competitive with the reference points. 
%\joerg{statement where coverage for these starts to go down} 
Trap learning is highly
beneficial in all cases except Blocksworld with $x>1.0$. Overall, it
seems fair to say that our action-set property dependency analysis is
not exceedingly infeasible compared to related classical planning
problems.


\fi

%%%% END PRE-FINAL AND SUPPLEMENTARY MATERIAL VERSION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





























