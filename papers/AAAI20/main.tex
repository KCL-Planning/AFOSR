\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in
% The file ijcai19.sty is NOT the same than previous years'
\usepackage{aaai19}

\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{url}  %Required
\usepackage{graphicx}  %Required

%%%%%%%%%%%%%%%%%%%%%
%%%% Packages
\usepackage{amsthm,amsmath,amssymb}
\usepackage{tikz}
\usetikzlibrary{matrix}
\usepackage{pgf-umlsd}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepgfplotslibrary{fillbetween}
\usepackage{ragged2e}
\usetikzlibrary{arrows,automata,fit,calc,positioning,shapes,shapes.multipart} %
%\usepackage[inline]{enumitem}
\usepackage{enumerate,url,soul}
\usepackage{paralist}
%\usepackage[usenames]{color} % Only used in comment commands
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{thmtools,thm-restate}
\usepackage{listings}
\usepackage{subcaption}
\usepackage{arydshln}

\usepackage{wrapfig}

%\setlength\floatsep{0.25\baselineskip}
%\setlength\textfloatsep{0.25\baselineskip}
%\setlength\intextsep{0.25\baselineskip}

\input{commands.tex}


\begin{document}

\title{Explaining the Space of Plans through Plan-Property Dependencies}

%\author{Submission \#????}

\author{
	Rebecca Eifler$^1$\and
	Michael Cashmore$^2$\and
	J\"org Hoffmann$^1$\and
	Daniele Magazzeni$^2$\and
	Marcel Steinmetz$^1$\\
	%\affiliations
	$^1$Saarland University, Saarland Informatics Campus, Saarbr\"ucken, Germany\\
	$^2$King's College London, Department of Informatics, London, UK\\
	%\emails
	\{lastname\}@cs.uni-saarland.de,
	\{firstname.lastname\}@kcl.ac.uk 
}

\maketitle

\begin{abstract}
A key problem in explainable AI planning is to elucidate decision
rationales. User questions in this context are often contrastive,
taking the form ``Why do A rather than B?'' \joerg{bit of a mismatch
  between this and the ``properties'' B and C below}. Answering such a
question requires a statement about the space of possible plans. We
propose to do so through plan-property dependencies, where plan
properties are Boolean properties of plans the user is interested in,
and dependencies are entailment relations in plan space. The answer to
the above question then consists of those properties C entailed by
B. We introduce a formal framework for such dependency analysis. We
instantiate and operationalize that framework for the case of
dependencies between goals in oversubscription planning.
%
% Joerg: the nogood learning is after all not super important for
% performance; whereas the compilations really are central to this
% paper.
%
%% , leveraging and extending recent nogood learning methods for
%% computational efficiency. We show that more general plan properties
%% can be compiled into this special case.
%
More powerful plan properties can be compiled into that special case.
%
We show experimentally that, in a variety of benchmarks, the suggested
analyses can be feasible and produce compact answers for human
inspection.
%
%% A key problem in explainable AI planning is to answer user questions
%% asking to elucidate decision rationales, \eg, ``Why do A rather than
%% B?'' This requires an explanation of the space of possible plans. We
%% propose to address this through the analysis of a set of plan
%% properties, identifying their dependencies in terms of entailment
%% relations in plan space. We introduce a generic formal framework for
%% such dependency analysis. We instantiate and operationalize that
%% framework for the case of dependencies between goals in
%% oversubscription planning.
%% %
%% % Joerg: the nogood learning is after all not super important for
%% % performance; whereas the compilations really are central to this
%% % paper.
%% %
%% %% , leveraging and extending recent nogood learning methods for
%% %% computational efficiency. We show that more general plan properties
%% %% can be compiled into this special case.
%% %
%% More powerful plan properties can be compiled into that special case.
%% %
%% We show experimentally that, in a variety of benchmarks, the suggested
%% analyses can be feasible and produce compact answers for human
%% inspection.
\end{abstract}


\input{introduction}

\input{framework}

\input{goaldep}

\input{compilation}

\input{illustrative-example}

\input{experiments}

%\input{related}

\input{conclusion}

%\input{examples}

%% \section*{Acknowledgments}
%
%% This material is based upon work supported by the Air Force Office
%% of Scientific Research under award number FA9550-18-1-0245. J\"org
%% Hoffmann's group has also received support by the German Research
%% Foundation (DFG) as part of CRC 248 (see
%% perspicuous-computing.science). Part of this work was performed
%% while J\"org Hoffmann was visiting NASA Ames Research Center. We
%% thank J.\ Benton, Minh Do, Jeremy Frank, and David Smith for
%% insightful discussions.

\ifdefined\longflagdefined

\bibliographystyle{named}
\bibliography{abbreviations,biblio,crossref}

\else 

\bibliographystyle{named}
\bibliography{abbreviations,biblio,crossref-short}

\fi





\ifdefined\suppflagdefined

\appendix

\input{comments_appendix.tex}
\input{data-action-set-properties}

\fi



\end{document}
